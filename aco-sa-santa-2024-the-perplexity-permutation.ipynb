{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":10249020,"sourceType":"datasetVersion","datasetId":6339117},{"sourceId":10249719,"sourceType":"datasetVersion","datasetId":6339648},{"sourceId":210460487,"sourceType":"kernelVersion"},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Evaluates text submissions.\n\nThis code defines a scoring system for a competition that evaluates text submissions. It calculates the mean perplexity of submitted text permutations compared to original texts using a pre-trained language model. It ensures that the submissions are valid permutations and uses a custom PerplexityCalculator class to compute perplexity scores. Lower scores indicate better quality submissions.\n","metadata":{}},{"cell_type":"code","source":"import gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = True,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str\n        Path to the serialized LLM.\n\n    clear_mem : bool\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path,padding_side=\"right\")\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n                \n            #quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            #quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True)\n\n            quantization_config = transformers.BitsAndBytesConfig(\n                load_in_4bit = True,\n                bnb_4bit_quant_type = \"fp4\", #fp4 nf4\n                bnb_4bit_use_double_quant = False,\n                bnb_4bit_compute_dtype=torch.float16,\n            )\n            \n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n        #if not load_in_8bit:\n        #    self.model.to(DEVICE)  # Explicitly move the model to the device\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], batch_size: 32\n    ) -> Union[float, List[float]]:\n        \"\"\"\n        Calculates the perplexity of given texts.\n\n        Parameters\n        ----------\n        input_texts : str or list of str\n            A single string or a list of strings.\n\n        batch_size : int, default=None\n            Batch size for processing. Defaults to the number of input texts.\n\n        verbose : bool, default=False\n            Display progress bar.\n\n        Returns\n        -------\n        float or list of float\n            A single perplexity value if input is a single string,\n            or a list of perplexity values if input is a list of strings.\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n        >>> scorer = PerplexityCalculator(model_path=model_path)\n\n        >>> submission = pd.DataFrame({\n        ...     'id': [0, 1, 2],\n        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n        ... })\n        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        >>> perplexities[0] < perplexities[1]\n        True\n        >>> perplexities[2] < perplexities[0]\n        True\n\n        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n        >>> all(p > 0 for p in perplexities)\n        True\n\n        >>> scorer.clear_gpu_memory()\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n\n        batches = len(input_texts)//batch_size + (len(input_texts)%batch_size != 0)\n        for j in range(batches):\n            \n            a = j*batch_size\n            b = (j+1)*batch_size\n            input_batch = input_texts[a:b]\n        \n            with torch.no_grad():\n\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = [f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\" for text in input_batch]\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                    padding=True\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                label = model_inputs['input_ids']\n                label[label == self.tokenizer.pad_token_id] = PAD_TOKEN_LABEL_ID\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = label[..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                loss = loss.view(len(logits), -1)\n                valid_length = (shift_labels != PAD_TOKEN_LABEL_ID).sum(dim=-1)\n                loss = torch.sum(loss, -1) / valid_length\n\n                loss_list += loss.cpu().tolist()\n\n                # Debug output\n                #print(f\"\\nProcessing: '{text}'\")\n                #print(f\"With special tokens: '{text_with_special}'\")\n                #print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                #print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                #print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                #print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                #print(f\"Individual losses: {loss.tolist()}\")\n                #print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        # print(\"\\nFinal perplexities:\")\n        # for text, perp in zip(input_texts, ppl):\n        #     print(f\"Text: '{text}'\")\n        #     print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-21T21:07:43.458506Z","iopub.execute_input":"2024-12-21T21:07:43.458809Z","iopub.status.idle":"2024-12-21T21:07:43.478524Z","shell.execute_reply.started":"2024-12-21T21:07:43.458784Z","shell.execute_reply":"2024-12-21T21:07:43.477597Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import itertools, math\n\ndf = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\ndf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:44:40.764266Z","iopub.execute_input":"2024-12-20T18:44:40.765001Z","iopub.status.idle":"2024-12-20T18:44:40.789146Z","shell.execute_reply.started":"2024-12-20T18:44:40.764961Z","shell.execute_reply":"2024-12-20T18:44:40.788104Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   id                                               text\n0   0  advent chimney elf family fireplace gingerbrea...\n1   1  advent chimney elf family fireplace gingerbrea...\n2   2  yuletide decorations gifts cheer holiday carol...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>advent chimney elf family fireplace gingerbrea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>advent chimney elf family fireplace gingerbrea...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>yuletide decorations gifts cheer holiday carol...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# implementation of Ant Colony Optimization\nThis code implements an Ant Colony Optimization (ACO) algorithm for optimizing word permutations based on perplexity scores using a language model. It iteratively finds the best permutation of words to minimize perplexity, incorporates early stopping based on performance, and includes a solver function for processing text data for competitions like the Santa 2024 Perplexity Permutation Puzzle.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport itertools\nimport random\n\nclass PerplexityPermutationACO:\n    def __init__(\n        self, \n        scorer, \n        original_text, \n        n_ants=50, \n        n_iterations=100, \n        alpha=1.0, \n        beta=2.0, \n        evaporation_rate=0.5,\n        batch_size=32,\n        patience=6  # New parameter for early stopping\n    ):\n        \"\"\"\n        Initialize the Ant Colony Optimization algorithm for permutation optimization.\n        \n        Parameters:\n        -----------\n        scorer : PerplexityCalculator\n            The perplexity scoring model\n        original_text : str\n            The original text to be permuted\n        n_ants : int\n            Number of ants (solutions) in each iteration\n        n_iterations : int\n            Maximum number of iterations to run the optimization\n        alpha : float\n            Pheromone importance factor\n        beta : float\n            Heuristic importance factor\n        evaporation_rate : float\n            Rate of pheromone evaporation\n        batch_size : int\n            Batch size for perplexity calculation\n        patience : int\n            Number of iterations to wait for improvement before stopping\n        \"\"\"\n        # Add patience to instance attributes\n        self.patience = patience\n        self.batch_size = batch_size\n\n        self.words = original_text.split()\n        self.n_words = len(self.words)\n        self.scorer = scorer\n        \n        # Optimization parameters\n        self.n_ants = n_ants\n        self.n_iterations = n_iterations\n        self.alpha = alpha\n        self.beta = beta\n        self.evaporation_rate = evaporation_rate\n        \n        # Initialize pheromone matrix\n        self.pheromone = np.ones((self.n_words, self.n_words)) / self.n_words\n        \n        # Best solution tracking\n        self.best_permutation = None\n        self.best_perplexity = float('inf')\n    \n    def _construct_solution(self):\n        \"\"\"\n        Construct a solution using probabilistic selection based on pheromones.\n        \n        Returns:\n        --------\n        list\n            A permuted list of words\n        \"\"\"\n        solution = []\n        available_indices = list(range(self.n_words))\n        \n        while available_indices:\n            # Compute selection probabilities\n            probabilities = np.zeros(len(available_indices))\n            for j, idx in enumerate(available_indices):\n                # Compute probability using pheromone and heuristic information\n                pheromone_factor = self.pheromone[len(solution), idx] ** self.alpha\n                probabilities[j] = pheromone_factor\n            \n            # Normalize probabilities\n            probabilities /= probabilities.sum()\n            \n            # Select word probabilistically\n            selected_index = np.random.choice(len(available_indices), p=probabilities)\n            selected_word_index = available_indices.pop(selected_index)\n            solution.append(self.words[selected_word_index])\n        \n        return solution\n    \n    def _update_pheromones(self, solutions, perplexities):\n        \"\"\"\n        Update pheromone levels based on solution quality.\n        \n        Parameters:\n        -----------\n        solutions : list of lists\n            Generated word permutations\n        perplexities : list\n            Corresponding perplexity scores\n        \"\"\"\n        # Decay existing pheromones\n        self.pheromone *= (1 - self.evaporation_rate)\n        \n        # Sort solutions by perplexity\n        sorted_solutions = sorted(zip(solutions, perplexities), key=lambda x: x[1])\n        \n        # Deposit more pheromone for better solutions\n        for (solution, perplexity), quality_factor in zip(\n            sorted_solutions, \n            np.linspace(1, 0.1, len(sorted_solutions))\n        ):\n            for i, word_index in enumerate(solution):\n                original_index = self.words.index(word_index)\n                self.pheromone[i, original_index] += quality_factor / perplexity\n    \n    def optimize(self):\n        \"\"\"\n        Run the Ant Colony Optimization algorithm with early stopping.\n        \n        Returns:\n        --------\n        tuple\n            Best permutation and its perplexity\n        \"\"\"\n        # Track iterations without improvement\n        iterations_without_improvement = 0\n        previous_best_perplexity = float('inf')\n        \n        for iteration in range(self.n_iterations):\n            # Generate solutions\n            solutions = []\n            perplexities = []\n            \n            for _ in range(self.n_ants):\n                solution = self._construct_solution()\n                solution_text = \" \".join(solution)\n                \n                # Use batch_size parameter when calling get_perplexity\n                perplexity = self.scorer.get_perplexity(\n                    solution_text, \n                    batch_size=self.batch_size\n                )\n                \n                solutions.append(solution)\n                perplexities.append(perplexity)\n                \n                # Update global best solution\n                if perplexity < self.best_perplexity:\n                    self.best_permutation = solution\n                    self.best_perplexity = perplexity\n            \n            # Update pheromone levels\n            self._update_pheromones(solutions, perplexities)\n            \n            # Check for improvement\n            if self.best_perplexity < previous_best_perplexity:\n                iterations_without_improvement = 0\n                previous_best_perplexity = self.best_perplexity\n            else:\n                iterations_without_improvement += 1\n            \n            # Optional: print progress\n            print(f\"Iteration {iteration+1}/{self.n_iterations}: \"\n                  f\"Best Perplexity = {self.best_perplexity:.4f}\")\n            \n            # Early stopping condition\n            if iterations_without_improvement >= self.patience:\n                print(f\"Early stopping triggered after {iteration+1} iterations.\")\n                break\n        \n        return self.best_permutation, self.best_perplexity\n\ndef solve_santa_2024_permutation(\n    sample_submission_path, \n    model_path='/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    output_path='submission.csv'\n):\n    \"\"\"\n    Solve the Santa 2024 Perplexity Permutation Puzzle using ACO.\n    \n    Parameters:\n    -----------\n    sample_submission_path : str\n        Path to the sample submission CSV\n    model_path : str\n        Path to the Gemma model\n    output_path : str\n        Path to save the output submission\n    \"\"\"\n    # Initialize scorer\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    \n    scorer = PerplexityCalculator(model_path)\n    \n    # Load sample submission\n    df = pd.read_csv(sample_submission_path)\n    \n    # Process each row\n    results = []\n    for idx, row in df.iterrows():\n        original_text = row['text']\n        \n        # Initialize and run ACO with early stopping\n        aco = PerplexityPermutationACO(\n            scorer=scorer, \n            original_text=original_text,\n            n_ants=50,\n            n_iterations=100,\n            batch_size=32,\n            patience=10  # Add patience for early stopping\n        )\n        \n        best_permutation, best_perplexity = aco.optimize()\n        \n        # Store result\n        results.append({\n            'id': idx,\n            'text': ' '.join(best_permutation)\n        })\n        \n        print(f\"Row {idx}: Best Perplexity = {best_perplexity:.4f}\")\n        \n        # Optional: free up GPU memory after each iteration\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    \n    # Create submission DataFrame\n    submission_df = pd.DataFrame(results)\n    submission_df.to_csv(output_path, index=False)\n    print(f\"Submission saved to {output_path}\")\n    \n    return submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:58:19.103587Z","iopub.execute_input":"2024-12-20T18:58:19.103902Z","iopub.status.idle":"2024-12-20T18:58:19.120355Z","shell.execute_reply.started":"2024-12-20T18:58:19.103877Z","shell.execute_reply":"2024-12-20T18:58:19.119377Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Example usage\nsolve_santa_2024_permutation('/kaggle/input/santa-2024/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:58:24.852051Z","iopub.execute_input":"2024-12-20T18:58:24.853201Z","iopub.status.idle":"2024-12-20T21:40:42.991122Z","shell.execute_reply.started":"2024-12-20T18:58:24.853148Z","shell.execute_reply":"2024-12-20T21:40:42.990192Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37b3f19fddf942e0938adee41cddc200"}},"metadata":{}},{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1/100: Best Perplexity = 1230.2282\nIteration 2/100: Best Perplexity = 1035.5767\nIteration 3/100: Best Perplexity = 965.4243\nIteration 4/100: Best Perplexity = 965.4243\nIteration 5/100: Best Perplexity = 965.4243\nIteration 6/100: Best Perplexity = 965.4243\nIteration 7/100: Best Perplexity = 965.4243\nIteration 8/100: Best Perplexity = 837.7315\nIteration 9/100: Best Perplexity = 626.9516\nIteration 10/100: Best Perplexity = 626.9516\nIteration 11/100: Best Perplexity = 626.9516\nIteration 12/100: Best Perplexity = 626.9516\nIteration 13/100: Best Perplexity = 626.9516\nIteration 14/100: Best Perplexity = 626.9516\nIteration 15/100: Best Perplexity = 618.5973\nIteration 16/100: Best Perplexity = 578.3719\nIteration 17/100: Best Perplexity = 578.3719\nIteration 18/100: Best Perplexity = 578.3719\nIteration 19/100: Best Perplexity = 578.3719\nIteration 20/100: Best Perplexity = 559.0029\nIteration 21/100: Best Perplexity = 559.0029\nIteration 22/100: Best Perplexity = 534.1312\nIteration 23/100: Best Perplexity = 534.1312\nIteration 24/100: Best Perplexity = 534.1312\nIteration 25/100: Best Perplexity = 534.1312\nIteration 26/100: Best Perplexity = 534.1312\nIteration 27/100: Best Perplexity = 534.1312\nIteration 28/100: Best Perplexity = 534.1312\nIteration 29/100: Best Perplexity = 534.1312\nIteration 30/100: Best Perplexity = 534.1312\nIteration 31/100: Best Perplexity = 534.1312\nIteration 32/100: Best Perplexity = 534.1312\nEarly stopping triggered after 32 iterations.\nRow 0: Best Perplexity = 534.1312\nIteration 1/100: Best Perplexity = 2536.7373\nIteration 2/100: Best Perplexity = 1639.0572\nIteration 3/100: Best Perplexity = 1639.0572\nIteration 4/100: Best Perplexity = 1639.0572\nIteration 5/100: Best Perplexity = 1639.0572\nIteration 6/100: Best Perplexity = 1639.0572\nIteration 7/100: Best Perplexity = 1639.0572\nIteration 8/100: Best Perplexity = 1639.0572\nIteration 9/100: Best Perplexity = 1639.0572\nIteration 10/100: Best Perplexity = 1639.0572\nIteration 11/100: Best Perplexity = 1639.0572\nIteration 12/100: Best Perplexity = 1603.7620\nIteration 13/100: Best Perplexity = 1603.7620\nIteration 14/100: Best Perplexity = 1603.7620\nIteration 15/100: Best Perplexity = 1597.0960\nIteration 16/100: Best Perplexity = 1536.1092\nIteration 17/100: Best Perplexity = 1274.3169\nIteration 18/100: Best Perplexity = 1274.3169\nIteration 19/100: Best Perplexity = 1274.3169\nIteration 20/100: Best Perplexity = 1274.3169\nIteration 21/100: Best Perplexity = 1235.5321\nIteration 22/100: Best Perplexity = 1235.5321\nIteration 23/100: Best Perplexity = 1235.5321\nIteration 24/100: Best Perplexity = 1235.5321\nIteration 25/100: Best Perplexity = 1235.5321\nIteration 26/100: Best Perplexity = 1235.5321\nIteration 27/100: Best Perplexity = 1224.5710\nIteration 28/100: Best Perplexity = 1224.5710\nIteration 29/100: Best Perplexity = 1224.5710\nIteration 30/100: Best Perplexity = 1224.5710\nIteration 31/100: Best Perplexity = 1224.5710\nIteration 32/100: Best Perplexity = 1224.5710\nIteration 33/100: Best Perplexity = 1204.1960\nIteration 34/100: Best Perplexity = 1157.1024\nIteration 35/100: Best Perplexity = 1157.1024\nIteration 36/100: Best Perplexity = 1157.1024\nIteration 37/100: Best Perplexity = 1157.1024\nIteration 38/100: Best Perplexity = 1149.8482\nIteration 39/100: Best Perplexity = 1149.8482\nIteration 40/100: Best Perplexity = 1042.7253\nIteration 41/100: Best Perplexity = 1042.7253\nIteration 42/100: Best Perplexity = 1042.7253\nIteration 43/100: Best Perplexity = 1042.7253\nIteration 44/100: Best Perplexity = 1042.7253\nIteration 45/100: Best Perplexity = 1042.7253\nIteration 46/100: Best Perplexity = 1042.7253\nIteration 47/100: Best Perplexity = 1042.7253\nIteration 48/100: Best Perplexity = 1042.7253\nIteration 49/100: Best Perplexity = 1042.7253\nIteration 50/100: Best Perplexity = 1042.7253\nEarly stopping triggered after 50 iterations.\nRow 1: Best Perplexity = 1042.7253\nIteration 1/100: Best Perplexity = 1132.5673\nIteration 2/100: Best Perplexity = 1132.5673\nIteration 3/100: Best Perplexity = 1132.5673\nIteration 4/100: Best Perplexity = 1132.5673\nIteration 5/100: Best Perplexity = 953.6955\nIteration 6/100: Best Perplexity = 953.6955\nIteration 7/100: Best Perplexity = 953.6955\nIteration 8/100: Best Perplexity = 932.8527\nIteration 9/100: Best Perplexity = 901.0676\nIteration 10/100: Best Perplexity = 841.6449\nIteration 11/100: Best Perplexity = 841.6449\nIteration 12/100: Best Perplexity = 841.6449\nIteration 13/100: Best Perplexity = 841.6449\nIteration 14/100: Best Perplexity = 841.6449\nIteration 15/100: Best Perplexity = 841.6449\nIteration 16/100: Best Perplexity = 810.9589\nIteration 17/100: Best Perplexity = 742.0658\nIteration 18/100: Best Perplexity = 705.5325\nIteration 19/100: Best Perplexity = 705.5325\nIteration 20/100: Best Perplexity = 673.5092\nIteration 21/100: Best Perplexity = 673.5092\nIteration 22/100: Best Perplexity = 673.5092\nIteration 23/100: Best Perplexity = 673.5092\nIteration 24/100: Best Perplexity = 673.5092\nIteration 25/100: Best Perplexity = 673.5092\nIteration 26/100: Best Perplexity = 673.5092\nIteration 27/100: Best Perplexity = 669.7520\nIteration 28/100: Best Perplexity = 669.7520\nIteration 29/100: Best Perplexity = 669.7520\nIteration 30/100: Best Perplexity = 654.7243\nIteration 31/100: Best Perplexity = 654.7243\nIteration 32/100: Best Perplexity = 610.9134\nIteration 33/100: Best Perplexity = 610.9134\nIteration 34/100: Best Perplexity = 610.9134\nIteration 35/100: Best Perplexity = 610.9134\nIteration 36/100: Best Perplexity = 606.5050\nIteration 37/100: Best Perplexity = 559.7762\nIteration 38/100: Best Perplexity = 559.7762\nIteration 39/100: Best Perplexity = 559.7762\nIteration 40/100: Best Perplexity = 551.9548\nIteration 41/100: Best Perplexity = 551.1610\nIteration 42/100: Best Perplexity = 550.6191\nIteration 43/100: Best Perplexity = 550.6191\nIteration 44/100: Best Perplexity = 550.6191\nIteration 45/100: Best Perplexity = 550.6191\nIteration 46/100: Best Perplexity = 545.5997\nIteration 47/100: Best Perplexity = 545.5997\nIteration 48/100: Best Perplexity = 545.5997\nIteration 49/100: Best Perplexity = 545.5997\nIteration 50/100: Best Perplexity = 545.5997\nIteration 51/100: Best Perplexity = 545.5997\nIteration 52/100: Best Perplexity = 545.5997\nIteration 53/100: Best Perplexity = 545.5997\nIteration 54/100: Best Perplexity = 545.5997\nIteration 55/100: Best Perplexity = 545.5997\nIteration 56/100: Best Perplexity = 545.5997\nEarly stopping triggered after 56 iterations.\nRow 2: Best Perplexity = 545.5997\nIteration 1/100: Best Perplexity = 1835.4285\nIteration 2/100: Best Perplexity = 1484.4402\nIteration 3/100: Best Perplexity = 1484.4402\nIteration 4/100: Best Perplexity = 1484.4402\nIteration 5/100: Best Perplexity = 1280.2320\nIteration 6/100: Best Perplexity = 1280.2320\nIteration 7/100: Best Perplexity = 1280.2320\nIteration 8/100: Best Perplexity = 1280.2320\nIteration 9/100: Best Perplexity = 1280.2320\nIteration 10/100: Best Perplexity = 1280.2320\nIteration 11/100: Best Perplexity = 1194.6881\nIteration 12/100: Best Perplexity = 1060.5988\nIteration 13/100: Best Perplexity = 1060.5988\nIteration 14/100: Best Perplexity = 1060.5988\nIteration 15/100: Best Perplexity = 1060.5988\nIteration 16/100: Best Perplexity = 1060.5988\nIteration 17/100: Best Perplexity = 1060.5988\nIteration 18/100: Best Perplexity = 1060.5988\nIteration 19/100: Best Perplexity = 989.9175\nIteration 20/100: Best Perplexity = 740.9372\nIteration 21/100: Best Perplexity = 740.9372\nIteration 22/100: Best Perplexity = 740.9372\nIteration 23/100: Best Perplexity = 740.9372\nIteration 24/100: Best Perplexity = 740.9372\nIteration 25/100: Best Perplexity = 740.9372\nIteration 26/100: Best Perplexity = 740.9372\nIteration 27/100: Best Perplexity = 740.9372\nIteration 28/100: Best Perplexity = 740.9372\nIteration 29/100: Best Perplexity = 740.9372\nIteration 30/100: Best Perplexity = 740.9372\nEarly stopping triggered after 30 iterations.\nRow 3: Best Perplexity = 740.9372\nIteration 1/100: Best Perplexity = 1229.7273\nIteration 2/100: Best Perplexity = 1229.7273\nIteration 3/100: Best Perplexity = 1131.7878\nIteration 4/100: Best Perplexity = 1025.2860\nIteration 5/100: Best Perplexity = 1025.2860\nIteration 6/100: Best Perplexity = 1025.2860\nIteration 7/100: Best Perplexity = 1025.2860\nIteration 8/100: Best Perplexity = 1025.2860\nIteration 9/100: Best Perplexity = 1025.2860\nIteration 10/100: Best Perplexity = 1025.2860\nIteration 11/100: Best Perplexity = 1025.2860\nIteration 12/100: Best Perplexity = 1025.2860\nIteration 13/100: Best Perplexity = 1025.2860\nIteration 14/100: Best Perplexity = 972.9535\nIteration 15/100: Best Perplexity = 972.9535\nIteration 16/100: Best Perplexity = 956.4941\nIteration 17/100: Best Perplexity = 956.4941\nIteration 18/100: Best Perplexity = 956.4941\nIteration 19/100: Best Perplexity = 956.4941\nIteration 20/100: Best Perplexity = 956.4941\nIteration 21/100: Best Perplexity = 956.4941\nIteration 22/100: Best Perplexity = 956.4941\nIteration 23/100: Best Perplexity = 934.3926\nIteration 24/100: Best Perplexity = 934.3926\nIteration 25/100: Best Perplexity = 934.3926\nIteration 26/100: Best Perplexity = 934.3926\nIteration 27/100: Best Perplexity = 920.6710\nIteration 28/100: Best Perplexity = 903.4700\nIteration 29/100: Best Perplexity = 903.4700\nIteration 30/100: Best Perplexity = 844.0020\nIteration 31/100: Best Perplexity = 844.0020\nIteration 32/100: Best Perplexity = 844.0020\nIteration 33/100: Best Perplexity = 844.0020\nIteration 34/100: Best Perplexity = 844.0020\nIteration 35/100: Best Perplexity = 844.0020\nIteration 36/100: Best Perplexity = 840.2843\nIteration 37/100: Best Perplexity = 840.2843\nIteration 38/100: Best Perplexity = 840.2843\nIteration 39/100: Best Perplexity = 840.2843\nIteration 40/100: Best Perplexity = 795.8373\nIteration 41/100: Best Perplexity = 795.8373\nIteration 42/100: Best Perplexity = 795.8373\nIteration 43/100: Best Perplexity = 786.9316\nIteration 44/100: Best Perplexity = 786.9316\nIteration 45/100: Best Perplexity = 786.9316\nIteration 46/100: Best Perplexity = 761.8040\nIteration 47/100: Best Perplexity = 761.8040\nIteration 48/100: Best Perplexity = 761.8040\nIteration 49/100: Best Perplexity = 754.6900\nIteration 50/100: Best Perplexity = 754.3852\nIteration 51/100: Best Perplexity = 754.3852\nIteration 52/100: Best Perplexity = 715.6602\nIteration 53/100: Best Perplexity = 715.6602\nIteration 54/100: Best Perplexity = 715.6602\nIteration 55/100: Best Perplexity = 715.6602\nIteration 56/100: Best Perplexity = 715.6602\nIteration 57/100: Best Perplexity = 663.8207\nIteration 58/100: Best Perplexity = 663.8207\nIteration 59/100: Best Perplexity = 663.8207\nIteration 60/100: Best Perplexity = 663.8207\nIteration 61/100: Best Perplexity = 663.8207\nIteration 62/100: Best Perplexity = 658.6463\nIteration 63/100: Best Perplexity = 658.6463\nIteration 64/100: Best Perplexity = 658.6463\nIteration 65/100: Best Perplexity = 658.6463\nIteration 66/100: Best Perplexity = 658.6463\nIteration 67/100: Best Perplexity = 658.6463\nIteration 68/100: Best Perplexity = 658.6463\nIteration 69/100: Best Perplexity = 658.6463\nIteration 70/100: Best Perplexity = 658.6463\nIteration 71/100: Best Perplexity = 658.6463\nIteration 72/100: Best Perplexity = 658.6463\nEarly stopping triggered after 72 iterations.\nRow 4: Best Perplexity = 658.6463\nIteration 1/100: Best Perplexity = 717.0282\nIteration 2/100: Best Perplexity = 717.0282\nIteration 3/100: Best Perplexity = 717.0282\nIteration 4/100: Best Perplexity = 717.0282\nIteration 5/100: Best Perplexity = 717.0282\nIteration 6/100: Best Perplexity = 675.0992\nIteration 7/100: Best Perplexity = 675.0992\nIteration 8/100: Best Perplexity = 647.1620\nIteration 9/100: Best Perplexity = 647.1620\nIteration 10/100: Best Perplexity = 647.1620\nIteration 11/100: Best Perplexity = 647.1620\nIteration 12/100: Best Perplexity = 647.1620\nIteration 13/100: Best Perplexity = 647.1620\nIteration 14/100: Best Perplexity = 639.1074\nIteration 15/100: Best Perplexity = 639.1074\nIteration 16/100: Best Perplexity = 639.1074\nIteration 17/100: Best Perplexity = 639.1074\nIteration 18/100: Best Perplexity = 607.3495\nIteration 19/100: Best Perplexity = 607.3495\nIteration 20/100: Best Perplexity = 607.3495\nIteration 21/100: Best Perplexity = 585.4941\nIteration 22/100: Best Perplexity = 585.4941\nIteration 23/100: Best Perplexity = 585.4941\nIteration 24/100: Best Perplexity = 585.4941\nIteration 25/100: Best Perplexity = 585.4941\nIteration 26/100: Best Perplexity = 585.4941\nIteration 27/100: Best Perplexity = 585.4941\nIteration 28/100: Best Perplexity = 568.6296\nIteration 29/100: Best Perplexity = 568.6296\nIteration 30/100: Best Perplexity = 568.6296\nIteration 31/100: Best Perplexity = 568.6296\nIteration 32/100: Best Perplexity = 567.5152\nIteration 33/100: Best Perplexity = 567.5152\nIteration 34/100: Best Perplexity = 557.8303\nIteration 35/100: Best Perplexity = 557.8303\nIteration 36/100: Best Perplexity = 557.8303\nIteration 37/100: Best Perplexity = 557.8303\nIteration 38/100: Best Perplexity = 557.8303\nIteration 39/100: Best Perplexity = 557.8303\nIteration 40/100: Best Perplexity = 557.8303\nIteration 41/100: Best Perplexity = 532.5682\nIteration 42/100: Best Perplexity = 532.5682\nIteration 43/100: Best Perplexity = 532.5682\nIteration 44/100: Best Perplexity = 532.5682\nIteration 45/100: Best Perplexity = 532.5682\nIteration 46/100: Best Perplexity = 532.5682\nIteration 47/100: Best Perplexity = 532.5682\nIteration 48/100: Best Perplexity = 520.3168\nIteration 49/100: Best Perplexity = 520.3168\nIteration 50/100: Best Perplexity = 519.1055\nIteration 51/100: Best Perplexity = 519.1055\nIteration 52/100: Best Perplexity = 518.3153\nIteration 53/100: Best Perplexity = 518.3153\nIteration 54/100: Best Perplexity = 505.8912\nIteration 55/100: Best Perplexity = 505.8912\nIteration 56/100: Best Perplexity = 501.4159\nIteration 57/100: Best Perplexity = 501.4159\nIteration 58/100: Best Perplexity = 501.4159\nIteration 59/100: Best Perplexity = 501.4159\nIteration 60/100: Best Perplexity = 479.9158\nIteration 61/100: Best Perplexity = 479.9158\nIteration 62/100: Best Perplexity = 464.8304\nIteration 63/100: Best Perplexity = 464.8304\nIteration 64/100: Best Perplexity = 464.8220\nIteration 65/100: Best Perplexity = 464.8220\nIteration 66/100: Best Perplexity = 464.8220\nIteration 67/100: Best Perplexity = 441.4128\nIteration 68/100: Best Perplexity = 441.4128\nIteration 69/100: Best Perplexity = 441.4128\nIteration 70/100: Best Perplexity = 441.4128\nIteration 71/100: Best Perplexity = 441.4128\nIteration 72/100: Best Perplexity = 441.4128\nIteration 73/100: Best Perplexity = 441.4128\nIteration 74/100: Best Perplexity = 441.4128\nIteration 75/100: Best Perplexity = 441.4128\nIteration 76/100: Best Perplexity = 441.4128\nIteration 77/100: Best Perplexity = 441.4128\nEarly stopping triggered after 77 iterations.\nRow 5: Best Perplexity = 441.4128\nSubmission saved to submission.csv\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   id                                               text\n0   0  reindeer mistletoe scrooge elf gingerbread orn...\n1   1  reindeer mistletoe scrooge gingerbread advent ...\n2   2  sleigh yuletide carol holly polar workshop gri...\n3   3  sleigh yuletide jingle sing of stocking naught...\n4   4  wreath believe paper it not merry have star yo...\n5   5  grinch unwrap card greeting eggnog the night e...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>reindeer mistletoe scrooge elf gingerbread orn...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>reindeer mistletoe scrooge gingerbread advent ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>sleigh yuletide carol holly polar workshop gri...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>sleigh yuletide jingle sing of stocking naught...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>wreath believe paper it not merry have star yo...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>grinch unwrap card greeting eggnog the night e...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"534.1312+1042.7253+545.5997+740.9372+658.6463+441.4128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T13:05:38.895261Z","iopub.execute_input":"2024-12-21T13:05:38.895524Z","iopub.status.idle":"2024-12-21T13:05:38.907120Z","shell.execute_reply.started":"2024-12-21T13:05:38.895495Z","shell.execute_reply":"2024-12-21T13:05:38.905984Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"3963.4525"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"3963.4525/6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T13:06:08.591953Z","iopub.execute_input":"2024-12-21T13:06:08.592762Z","iopub.status.idle":"2024-12-21T13:06:08.597893Z","shell.execute_reply.started":"2024-12-21T13:06:08.592730Z","shell.execute_reply":"2024-12-21T13:06:08.597039Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"660.5754166666667"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Result\nThe implemented Ant Colony Optimization (ACO) algorithm achieved a mean perplexity score of 660.575 across all processed rows, demonstrating its capability to optimize word permutations effectively for minimizing perplexity. To further enhance this performance, \n\nWe integrate Simulated Annealing, providing an additional layer of optimization to refine the results and potentially achieve better perplexity scores.","metadata":{}},{"cell_type":"markdown","source":"# Hybrid Optimization Algorithm: Simulated Annealing (SA) and Ant Colony Optimization (ACO)\n\nThis code implements a **Hybrid Optimization Algorithm** combining **Simulated Annealing (SA)** and **Ant Colony Optimization (ACO)** for solving permutation-based problems with a focus on optimizing perplexity, which is commonly used as a metric in language models. The hybrid approach leverages the strengths of both algorithms to improve the solution quality.\n\n---\n\n## **Key Features and Explanation**\n\n### **1. Hybrid Framework**\nThe optimizer combines:\n- **Simulated Annealing (SA):**\n    - Explores the solution space through probabilistic acceptance of worse solutions to escape local minima.\n    - Gradually cools down the \"temperature,\" reducing the likelihood of accepting worse solutions over time.\n- **ACO-Inspired Local Search:**\n    - Mimics the behavior of ants finding optimal paths using pheromone trails.\n    - Uses heuristics to refine solutions iteratively.\n\n---\n\n### **2. Parameters**\n\n#### **SA Parameters:**\n- **initial_temperature**: The starting temperature for SA.\n- **cooling_rate**: The rate at which the temperature decreases.\n\n#### **ACO Parameters:**\n- **n_ants**: Number of solutions (ants) evaluated per iteration.\n- **alpha** and **beta**: Weighting for pheromone importance versus heuristic factors.\n\n#### **Optimization Settings:**\n- **n_iterations**: Maximum number of iterations.\n- **batch_size**: Adjusted for perplexity calculation speed.\n- **early_stopping** and **patience**: Stop the algorithm early if no improvement is observed.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport random\nfrom typing import List, Tuple\n\nclass HybridPerplexityPermutationOptimizer:\n    def __init__(\n        self, \n        scorer, \n        original_text: str, \n        n_ants=30,               # Augmentation du nombre de fourmis\n        n_iterations=200,         # Plus d'itérations\n        alpha=0.5,                # Influence phéromones\n        beta=1,                 # Influence heuristiques\n        initial_temperature=30.0, # Température initiale réduite\n        cooling_rate=0.98,        # Refroidissement plus lent\n        batch_size=16,            # Réduction de la taille du batch pour accélérer\n        patience=10,              # Augmentation de la patience\n        early_stopping=True\n    ):\n        \"\"\"\n        Hybrid Ant Colony Optimization and Simulated Annealing for Permutation Optimization\n        \n        Parameters explained in previous implementations\n        \"\"\"\n        self.words = original_text.split()\n        self.n_words = len(self.words)\n        self.scorer = scorer\n        \n        # SA parameters\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n        \n        # ACO parameters\n        self.n_ants = n_ants\n        self.n_iterations = n_iterations\n        self.alpha = alpha\n        self.beta = beta\n        self.batch_size = batch_size\n        self.patience = patience\n        self.early_stopping = early_stopping\n        \n        # Pheromone matrix for ACO\n        self.pheromone = np.ones((self.n_words, self.n_words)) / self.n_words\n        \n        # Best solution tracking\n        self.best_permutation = None\n        self.best_perplexity = float('inf')\n        self.no_improvement_counter = 0\n    \n    def _evaluate_permutation(self, permutation: List[str]) -> float:\n        \"\"\"Compute perplexity for a single permutation\"\"\"\n        text = \" \".join(permutation)\n        return self.scorer.get_perplexity(text, batch_size=self.batch_size)\n    \n    def _initialize_permutation(self) -> List[str]:\n        \"\"\"Initialize a random permutation\"\"\"\n        return random.sample(self.words, len(self.words))\n    \n    def _neighbor(self, permutation: List[str]) -> List[str]:\n        \"\"\"Generate a neighboring solution by swapping two elements\"\"\"\n        neighbor = permutation.copy()\n        i, j = random.sample(range(len(permutation)), 2)\n        neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n        return neighbor\n    \n    def _simulated_annealing(self) -> Tuple[List[str], float]:\n        \"\"\"Simulated Annealing Optimization\"\"\"\n        current_solution = self._initialize_permutation()\n        current_perplexity = self._evaluate_permutation(current_solution)\n        best_solution = current_solution\n        best_perplexity = current_perplexity\n        \n        temperature = self.initial_temperature\n        \n        while temperature > 1e-3:\n            # Generate neighbor\n            neighbor_solution = self._neighbor(current_solution)\n            neighbor_perplexity = self._evaluate_permutation(neighbor_solution)\n            \n            # Acceptance probability\n            delta = neighbor_perplexity - current_perplexity\n            if delta < 0 or random.random() < np.exp(-delta / temperature):\n                current_solution = neighbor_solution\n                current_perplexity = neighbor_perplexity\n            \n            # Update best solution\n            if current_perplexity < best_perplexity:\n                best_solution = current_solution\n                best_perplexity = current_perplexity\n            \n            # Cool down\n            temperature *= self.cooling_rate\n        \n        return best_solution, best_perplexity\n\n    def _aco_local_search(self, permutation: List[str], perplexity: float) -> Tuple[List[str], float]:\n        \"\"\"ACO-inspired local search\"\"\"\n        best_permutation = permutation\n        best_perplexity = perplexity\n        for _ in range(3):  # Local search attempts\n            candidate = self._neighbor(permutation)\n            candidate_perplexity = self._evaluate_permutation(candidate)\n            if candidate_perplexity < best_perplexity:\n                best_permutation = candidate\n                best_perplexity = candidate_perplexity\n        return best_permutation, best_perplexity\n\n    def optimize(self) -> Tuple[List[str], float]:\n        \"\"\"Hybrid ACO-SA Optimization Process\"\"\"\n        best_solution, best_perplexity = self._simulated_annealing()\n        for iteration in range(self.n_iterations):\n            # ACO-inspired local search\n            best_solution, best_perplexity = self._aco_local_search(best_solution, best_perplexity)\n            print(f\"Iteration {iteration+1}/{self.n_iterations}: Best Perplexity = {best_perplexity:.4f}\")\n            \n            # Early stopping check\n            if self.early_stopping:\n                if best_perplexity < self.best_perplexity:\n                    self.best_perplexity = best_perplexity\n                    self.no_improvement_counter = 0\n                else:\n                    self.no_improvement_counter += 1\n                    if self.no_improvement_counter >= self.patience:\n                        print(\"Early stopping triggered.\")\n                        break\n        \n        return best_solution, best_perplexity\n\n\ndef solve_santa_2024_permutation_SA(\n    sample_submission_path: str, \n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    output_path: str = 'submission.csv'\n):\n    \"\"\"\n    Solve Santa 2024 Perplexity Permutation Puzzle using Hybrid ACO-SA\n    \"\"\"\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    \n    # Initialize scorer\n    scorer = PerplexityCalculator(model_path)\n    \n    # Load sample submission\n    df = pd.read_csv(sample_submission_path)\n    \n    # Process each row\n    results = []\n    for idx, row in df.iterrows():\n        original_text = row['text']\n        \n        # Initialize and run Hybrid Optimizer\n        optimizer = HybridPerplexityPermutationOptimizer(\n            scorer=scorer, \n            original_text=original_text,\n            early_stopping=True\n        )\n        \n        best_permutation, best_perplexity = optimizer.optimize()\n        \n        # Store result\n        results.append({\n            'id': idx,\n            'text': ' '.join(best_permutation)\n        })\n        \n        print(f\"Row {idx}: Best Perplexity = {best_perplexity:.4f}\")\n        \n        # Optional: Free GPU memory\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    \n    # Create submission DataFrame\n    submission_df = pd.DataFrame(results)\n    submission_df.to_csv(output_path, index=False)\n    print(f\"Submission saved to {output_path}\")\n    \n    return submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T21:07:53.401144Z","iopub.execute_input":"2024-12-21T21:07:53.401515Z","iopub.status.idle":"2024-12-21T21:07:53.418133Z","shell.execute_reply.started":"2024-12-21T21:07:53.401482Z","shell.execute_reply":"2024-12-21T21:07:53.417305Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Example usage\nsolve_santa_2024_permutation_SA('/kaggle/input/santa-2024/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T21:08:01.030942Z","iopub.execute_input":"2024-12-21T21:08:01.031602Z","iopub.status.idle":"2024-12-21T21:18:14.539554Z","shell.execute_reply.started":"2024-12-21T21:08:01.031570Z","shell.execute_reply":"2024-12-21T21:18:14.538753Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c51d35f09a41cfbe810ead51915608"}},"metadata":{}},{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1/200: Best Perplexity = 556.4528\nIteration 2/200: Best Perplexity = 556.4528\nIteration 3/200: Best Perplexity = 556.4528\nIteration 4/200: Best Perplexity = 556.4528\nIteration 5/200: Best Perplexity = 556.4528\nIteration 6/200: Best Perplexity = 556.4528\nIteration 7/200: Best Perplexity = 556.4528\nIteration 8/200: Best Perplexity = 556.4528\nIteration 9/200: Best Perplexity = 556.4528\nIteration 10/200: Best Perplexity = 556.4528\nIteration 11/200: Best Perplexity = 556.4528\nEarly stopping triggered.\nRow 0: Best Perplexity = 556.4528\nIteration 1/200: Best Perplexity = 852.1607\nIteration 2/200: Best Perplexity = 852.1607\nIteration 3/200: Best Perplexity = 852.1607\nIteration 4/200: Best Perplexity = 852.1607\nIteration 5/200: Best Perplexity = 852.1607\nIteration 6/200: Best Perplexity = 852.1607\nIteration 7/200: Best Perplexity = 852.1607\nIteration 8/200: Best Perplexity = 852.1607\nIteration 9/200: Best Perplexity = 852.1607\nIteration 10/200: Best Perplexity = 852.1607\nIteration 11/200: Best Perplexity = 852.1607\nEarly stopping triggered.\nRow 1: Best Perplexity = 852.1607\nIteration 1/200: Best Perplexity = 500.7958\nIteration 2/200: Best Perplexity = 500.7958\nIteration 3/200: Best Perplexity = 500.7958\nIteration 4/200: Best Perplexity = 500.7958\nIteration 5/200: Best Perplexity = 500.7958\nIteration 6/200: Best Perplexity = 500.7958\nIteration 7/200: Best Perplexity = 500.7958\nIteration 8/200: Best Perplexity = 500.7958\nIteration 9/200: Best Perplexity = 500.7958\nIteration 10/200: Best Perplexity = 500.7958\nIteration 11/200: Best Perplexity = 500.7958\nEarly stopping triggered.\nRow 2: Best Perplexity = 500.7958\nIteration 1/200: Best Perplexity = 442.9493\nIteration 2/200: Best Perplexity = 442.9493\nIteration 3/200: Best Perplexity = 442.9493\nIteration 4/200: Best Perplexity = 442.9493\nIteration 5/200: Best Perplexity = 442.9493\nIteration 6/200: Best Perplexity = 442.9493\nIteration 7/200: Best Perplexity = 442.9493\nIteration 8/200: Best Perplexity = 442.9493\nIteration 9/200: Best Perplexity = 442.9493\nIteration 10/200: Best Perplexity = 442.9493\nIteration 11/200: Best Perplexity = 442.9493\nEarly stopping triggered.\nRow 3: Best Perplexity = 442.9493\nIteration 1/200: Best Perplexity = 511.9707\nIteration 2/200: Best Perplexity = 511.9707\nIteration 3/200: Best Perplexity = 511.9707\nIteration 4/200: Best Perplexity = 501.2758\nIteration 5/200: Best Perplexity = 501.2758\nIteration 6/200: Best Perplexity = 501.2758\nIteration 7/200: Best Perplexity = 501.2758\nIteration 8/200: Best Perplexity = 501.2758\nIteration 9/200: Best Perplexity = 501.2758\nIteration 10/200: Best Perplexity = 501.2758\nIteration 11/200: Best Perplexity = 501.2758\nIteration 12/200: Best Perplexity = 501.2758\nIteration 13/200: Best Perplexity = 501.2758\nIteration 14/200: Best Perplexity = 501.2758\nEarly stopping triggered.\nRow 4: Best Perplexity = 501.2758\nIteration 1/200: Best Perplexity = 387.2697\nIteration 2/200: Best Perplexity = 387.2697\nIteration 3/200: Best Perplexity = 387.2697\nIteration 4/200: Best Perplexity = 378.3631\nIteration 5/200: Best Perplexity = 368.1399\nIteration 6/200: Best Perplexity = 368.1399\nIteration 7/200: Best Perplexity = 367.9440\nIteration 8/200: Best Perplexity = 367.9440\nIteration 9/200: Best Perplexity = 367.9440\nIteration 10/200: Best Perplexity = 367.9440\nIteration 11/200: Best Perplexity = 367.9440\nIteration 12/200: Best Perplexity = 367.9440\nIteration 13/200: Best Perplexity = 367.7118\nIteration 14/200: Best Perplexity = 366.4830\nIteration 15/200: Best Perplexity = 366.4830\nIteration 16/200: Best Perplexity = 366.4830\nIteration 17/200: Best Perplexity = 366.4830\nIteration 18/200: Best Perplexity = 366.4830\nIteration 19/200: Best Perplexity = 366.4830\nIteration 20/200: Best Perplexity = 366.4830\nIteration 21/200: Best Perplexity = 366.4830\nIteration 22/200: Best Perplexity = 358.2128\nIteration 23/200: Best Perplexity = 358.2128\nIteration 24/200: Best Perplexity = 358.2128\nIteration 25/200: Best Perplexity = 358.2128\nIteration 26/200: Best Perplexity = 358.2128\nIteration 27/200: Best Perplexity = 357.9567\nIteration 28/200: Best Perplexity = 357.9567\nIteration 29/200: Best Perplexity = 357.9567\nIteration 30/200: Best Perplexity = 357.9567\nIteration 31/200: Best Perplexity = 357.9567\nIteration 32/200: Best Perplexity = 357.9567\nIteration 33/200: Best Perplexity = 357.9567\nIteration 34/200: Best Perplexity = 357.9567\nIteration 35/200: Best Perplexity = 357.9567\nIteration 36/200: Best Perplexity = 354.2260\nIteration 37/200: Best Perplexity = 330.7505\nIteration 38/200: Best Perplexity = 330.7505\nIteration 39/200: Best Perplexity = 330.7505\nIteration 40/200: Best Perplexity = 328.5815\nIteration 41/200: Best Perplexity = 328.5815\nIteration 42/200: Best Perplexity = 328.5815\nIteration 43/200: Best Perplexity = 322.2915\nIteration 44/200: Best Perplexity = 322.2915\nIteration 45/200: Best Perplexity = 322.2915\nIteration 46/200: Best Perplexity = 322.2915\nIteration 47/200: Best Perplexity = 322.2915\nIteration 48/200: Best Perplexity = 322.2915\nIteration 49/200: Best Perplexity = 322.2915\nIteration 50/200: Best Perplexity = 322.2915\nIteration 51/200: Best Perplexity = 322.2915\nIteration 52/200: Best Perplexity = 322.2915\nIteration 53/200: Best Perplexity = 322.2915\nEarly stopping triggered.\nRow 5: Best Perplexity = 322.2915\nSubmission saved to submission.csv\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   id                                               text\n0   0  reindeer mistletoe scrooge gingerbread chimney...\n1   1  reindeer the elf scrooge family sleep walk nig...\n2   2  sleigh yuletide naughty nice holiday cheer gri...\n3   3  magi grinch holiday yuletide cheer relax unwra...\n4   4  merry hohoho it snowglobe toy game puzzle from...\n5   5  and hope joy the grinch relax eat unwrap give ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>reindeer mistletoe scrooge gingerbread chimney...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>reindeer the elf scrooge family sleep walk nig...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>sleigh yuletide naughty nice holiday cheer gri...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>magi grinch holiday yuletide cheer relax unwra...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>merry hohoho it snowglobe toy game puzzle from...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>and hope joy the grinch relax eat unwrap give ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"### Results:\n\n- **Row 0**: 556.4528\n- **Row 1**: 852.1607\n- **Row 2**: 500.7958\n- **Row 3**: 442.9493\n- **Row 4**: 501.2758\n- **Row 5**: 322.2915\n\nThe mean perplexity across all rows is approximately **529.32**. This indicates an overall improvement in the perplexity as the hybrid optimization algorithm progresses. The decrease in perplexity values across the rows, especially from Row 0 (556.45) to Row 5 (322.29), suggests that the hybrid approach effectively reduces perplexity, which is commonly associated with better model performance, likely leading to improved optimization and solution quality in the context of the problem being solved.\n","metadata":{}}]}